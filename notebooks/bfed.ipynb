{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  CIFAR10\n",
      "Parties:  16\n",
      "Identical Forgettable Distribution:  True\n",
      "Stratification:  True\n",
      "Data Boost Fraction:  0.0\n",
      "Batch Size:  64\n",
      "Communicative Rounds:  50\n",
      "Local Epochs:  20\n",
      "Model:  ResNet18\n",
      "Optimizer:  SGD\n",
      "Learning Rate:  0.01\n",
      "Momentum:  0.9\n",
      "Weight Decay:  0\n",
      "Seed:  42\n",
      "\n",
      "File name:  P16_ResNet18_BT0_BS64_R50_E20_FSTR_LSTR_S42\n"
     ]
    }
   ],
   "source": [
    "GPU = 0\n",
    "\n",
    "SEEDS = [0, 1, 42, 2 ** 8, 2 ** 16]\n",
    "SEED = SEEDS[2]    # Default 42\n",
    "\n",
    "# DATASETS = ['cifar10', 'mnist', 'permuted_mnist', 'fmnist']\n",
    "DATASETS = ['cifar10', 'mnist', 'fmnist']\n",
    "DATASET = DATASETS[0]    # Default cifar10\n",
    "\n",
    "PSET = [16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56]\n",
    "NUM_PARTIES = PSET[0]    # 0(16) ~ 11(56) # Default 16\n",
    "\n",
    "IFD, STRATIFY = True, True\n",
    "# IFD, STRATIFY = True, False\n",
    "# IFD, STRATIFY = False, False\n",
    "# IFD, STRATIFY = False, True\n",
    "\n",
    "BOOST_FRACS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# BOOST_FRACS = [0.25, 0.27, 0.29, 0.31, 0.33, 0.35]\n",
    "BOOST_FRAC = BOOST_FRACS[0]    # Default 0.3\n",
    "\n",
    "BATCH_SIZES = [64, 128, 256, 512, 1024]\n",
    "BATCH_SIZE = BATCH_SIZES[0]    # Default 64\n",
    "\n",
    "ROUNDSET = [30, 50, 100]\n",
    "ROUNDS = ROUNDSET[1]    # Default 50\n",
    "\n",
    "EPOCHSET = [10, 20, 30, 40, 50]\n",
    "EPOCHS = EPOCHSET[1]    # Default 20\n",
    "\n",
    "MODELS = ['MNISTLeNet5', 'LeNet5', 'VGG9', 'ResNet18', 'MobileNetV2', 'DenseNet', 'EfficientNet']\n",
    "MODEL = MODELS[3]    # Default VGG9\n",
    "\n",
    "OPTIMIZERS = ['sgd', 'adam']\n",
    "OPTIMIZER = OPTIMIZERS[0]    # Default sgd\n",
    "\n",
    "LRS = [0.1, 0.01, 0.001]\n",
    "LR = LRS[1]    # Default 0.01\n",
    "\n",
    "if OPTIMIZER == 'adam':\n",
    "    MOMENTUM = None\n",
    "elif OPTIMIZER == 'sgd':\n",
    "    MOMENTUM = 0.9\n",
    "    \n",
    "WDECAYS = [0, 0.0001]\n",
    "WEIGHT_DECAY = WDECAYS[0]    # Default 0.0001\n",
    "\n",
    "print('Dataset: ', DATASET.upper())\n",
    "print('Parties: ', NUM_PARTIES)\n",
    "print('Identical Forgettable Distribution: ', IFD)\n",
    "print('Stratification: ', STRATIFY)\n",
    "print('Data Boost Fraction: ', BOOST_FRAC)\n",
    "print('Batch Size: ', BATCH_SIZE)\n",
    "print('Communicative Rounds: ', ROUNDS)\n",
    "print('Local Epochs: ', EPOCHS)\n",
    "print('Model: ', MODEL)\n",
    "print('Optimizer: ', OPTIMIZER.upper())\n",
    "print('Learning Rate: ', LR)\n",
    "print('Momentum: ', MOMENTUM)\n",
    "print('Weight Decay: ', WEIGHT_DECAY)\n",
    "print('Seed: ', SEED)\n",
    "\n",
    "if IFD:\n",
    "    forget_dist = 'FSTR'\n",
    "else:\n",
    "    forget_dist = 'FRND'\n",
    "    \n",
    "if STRATIFY:\n",
    "    target_dist = 'LSTR'\n",
    "else:\n",
    "    target_dist = 'LRND'\n",
    "\n",
    "FNAME = f'P{NUM_PARTIES}_{MODEL}_BT{int(BOOST_FRAC * 10)}_BS{BATCH_SIZE}_R{ROUNDS}_E{EPOCHS}_{forget_dist}_{target_dist}_S{SEED}'\n",
    "\n",
    "print('\\nFile name: ', FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed = SEED\n",
    "np.random.seed = SEED\n",
    "torch.manual_seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{GPU}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Project Root:  /home/dev/projects/aaai20\n",
      "        Data Dir:  /home/dev/projects/aaai20/data\n",
      "     CIFAR10 Dir:  /home/dev/projects/aaai20/data/cifar10\n",
      " Party Split Dir:  /home/dev/projects/aaai20/data/cifar10/16parties\n",
      " Forget Dist Dir:  /home/dev/projects/aaai20/data/cifar10/16parties/ifd\n",
      "  Label Dist Dir:  /home/dev/projects/aaai20/data/cifar10/16parties/ifd/stratified\n",
      "        Save Dir:  /home/dev/projects/aaai20/saves\n",
      "CIFAR10 Save Dir:  /home/dev/projects/aaai20/saves/cifar10\n",
      "      Config Dir:  /home/dev/projects/aaai20/configs\n"
     ]
    }
   ],
   "source": [
    "PATH_ROOT = os.path.dirname(os.getcwd())\n",
    "\n",
    "PATH_DATA = os.path.join(PATH_ROOT, 'data')\n",
    "PATH_DSET = os.path.join(PATH_DATA, DATASET)\n",
    "PATH_PART = os.path.join(PATH_DSET, '{}parties'.format(NUM_PARTIES))\n",
    "PATH_FGD = os.path.join(PATH_PART, 'ifd') if IFD else os.path.join(PATH_PART, 'non_ifd')\n",
    "PATH_TGD = os.path.join(PATH_FGD, 'stratified') if STRATIFY else os.path.join(PATH_FGD, 'random')\n",
    "\n",
    "PATH_SAVE = os.path.join(PATH_ROOT, 'saves')\n",
    "SAVE_DSET = os.path.join(PATH_SAVE, DATASET)\n",
    "if not os.path.exists(SAVE_DSET):\n",
    "    os.mkdir(SAVE_DSET)\n",
    "    \n",
    "PATH_CONF = os.path.join(PATH_ROOT, 'configs')\n",
    "\n",
    "print('    Project Root: ', PATH_ROOT)\n",
    "print('        Data Dir: ', PATH_DATA)\n",
    "print(' {:>11} Dir: '.format(DATASET.upper()), PATH_DSET)\n",
    "print(' Party Split Dir: ', PATH_PART)\n",
    "print(' Forget Dist Dir: ', PATH_FGD)\n",
    "print('  Label Dist Dir: ', PATH_TGD)\n",
    "print('        Save Dir: ', PATH_SAVE)\n",
    "print('{:>7} Save Dir: '.format(DATASET.upper()), SAVE_DSET)\n",
    "print('      Config Dir: ', PATH_CONF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name:  P16_ResNet18_BT0_BS64_R50_E20_FSTR_LSTR_S42\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'seed': SEED,\n",
    "    'dataset': DATASET,\n",
    "    'num_parties': NUM_PARTIES,\n",
    "    'ifd': IFD,\n",
    "    'stratifiy': STRATIFY,\n",
    "    'boost_frac': BOOST_FRAC,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'comm_round': ROUNDS,\n",
    "    'local_epoch': EPOCHS,\n",
    "    'model': MODEL,\n",
    "    'optimizer': OPTIMIZER,\n",
    "    'lr': LR,\n",
    "    'momentum': MOMENTUM,\n",
    "    'weight_decay': WEIGHT_DECAY\n",
    "}\n",
    "\n",
    "with open('{}.json'.format(os.path.join(PATH_CONF, FNAME)), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=4, separators=(',', ': '))\n",
    "\n",
    "print('File name: ', FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Party  1] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  2] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  3] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  4] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  5] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  6] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  7] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  8] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party  9] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 10] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 11] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 12] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 13] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 14] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 15] Samples 3125, Forgettables 2218, Unforgettables 907\n",
      "[Party 16] Samples 3125, Forgettables 2218, Unforgettables 907\n"
     ]
    }
   ],
   "source": [
    "parties = []\n",
    "\n",
    "for i in range(1, NUM_PARTIES + 1):\n",
    "    df = pd.read_csv(os.path.join(PATH_TGD, '{}_p{}.csv'.format(DATASET, i)))\n",
    "    if BOOST_FRAC > 0:\n",
    "        boost = df.iloc[df[df['forgettable'] == 1].sample(frac=BOOST_FRAC).index]\n",
    "        df = df.append(boost, ignore_index=True)\n",
    "    parties.append(df)\n",
    "    print('[Party {:>2}] Samples {}, Forgettables {}, Unforgettables {}'.format(\n",
    "        i, len(df.index), len(df.loc[df['forgettable'] == 1].index), len(df.loc[df['forgettable'] == 0].index)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(PATH_DATA, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(PATH_DATA, train=False, transform=transform, download=False)\n",
    "    \n",
    "elif DATASET == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(PATH_DATA, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(PATH_DATA, train=False, transform=transform, download=False)\n",
    "    \n",
    "elif DATASET == 'fmnist':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.FashionMNIST(PATH_DATA, train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.FashionMNIST(PATH_DATA, train=False, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num workers:  0\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 0\n",
    "print('Num workers: ', NUM_WORKERS)\n",
    "\n",
    "trainloaders = []\n",
    "\n",
    "for p in parties:\n",
    "    train_subset = Subset(train_dataset, p['indices'].to_numpy())\n",
    "    trainloaders.append(DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS))\n",
    "    \n",
    "testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLeNet5(nn.Module):  # CNN\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 5 * 5 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.linear = nn.Linear(512 * ResidualBlock.expansion, 10)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        planes = expansion * in_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    cfg = [(1, 16, 1, 1),\n",
    "           (6, 24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "           (6, 32, 3, 2),\n",
    "           (6, 64, 4, 2),\n",
    "           (6, 96, 3, 1),\n",
    "           (6, 160, 3, 2),\n",
    "           (6, 320, 1, 1)]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(1280)\n",
    "        self.linear = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1] * (num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(MobileBlock(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(BottleNeck, num_planes, 6)\n",
    "        num_planes += 6 * growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(BottleNeck, num_planes, 12)\n",
    "        num_planes += 12 * growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(BottleNeck, num_planes, 24)\n",
    "        num_planes += 24 * growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(BottleNeck, num_planes, 16)\n",
    "        num_planes += 16 * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, criterion, device):\n",
    "    loss, correct, total = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        batch_ls = criterion(outputs, labels)\n",
    "        loss += batch_ls.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds = preds.view(-1)\n",
    "        correct += torch.sum(torch.eq(preds, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    loss /= len(testloader)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, momentum, weight_decay, criterion, device):\n",
    "    losses = []\n",
    "    \n",
    "    if momentum:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        batch_ls = []\n",
    "        \n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_ls.append(loss.item())\n",
    "\n",
    "        loss_avg = sum(batch_ls) / len(batch_ls)\n",
    "        losses.append(loss_avg)\n",
    "    return model.state_dict(), losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "     ResidualBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "    ResidualBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
      "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
      "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "    ResidualBlock-19          [-1, 128, 16, 16]               0\n",
      "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "    ResidualBlock-24          [-1, 128, 16, 16]               0\n",
      "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
      "    ResidualBlock-31            [-1, 256, 8, 8]               0\n",
      "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
      "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "    ResidualBlock-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
      "    ResidualBlock-43            [-1, 512, 4, 4]               0\n",
      "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "    ResidualBlock-48            [-1, 512, 4, 4]               0\n",
      "           Linear-49                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 11.25\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 53.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'mnist':\n",
    "    fed_model = MNISTLeNet5()\n",
    "elif MODEL == 'LeNet5':\n",
    "    fed_model = LeNet5()\n",
    "elif MODEL == 'VGG9':\n",
    "    fed_model = VGG9()\n",
    "elif MODEL == 'ResNet18':\n",
    "    fed_model = ResNet18()\n",
    "elif MODEL == 'MobileNetV2':\n",
    "    fed_model = MobileNetV2()\n",
    "elif MODEL == 'DenseNet':\n",
    "    fed_model = DenseNet()\n",
    "elif MODEL == 'EfficientNet':\n",
    "    fed_model = EfficientNet()\n",
    "    \n",
    "if DATASET == 'cifar10':\n",
    "    in_shape = (3, 32, 32)\n",
    "else:\n",
    "    in_shape = (1, 28, 28)\n",
    "    \n",
    "summary(fed_model, in_shape, device='cpu')\n",
    "\n",
    "fed_model.to(device)\n",
    "fed_weights = fed_model.state_dict()\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.4759 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.4311 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.4519 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.4376 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.4714 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.4236 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.4348 ... 20 local epochs\n",
      "  |-- [Party  8] Average Train Loss: 0.4726 ... 20 local epochs\n",
      "  |-- [Party  9] Average Train Loss: 0.4819 ... 20 local epochs\n",
      "  |-- [Party 10] Average Train Loss: 0.4132 ... 20 local epochs\n",
      "  |-- [Party 11] Average Train Loss: 0.4266 ... 20 local epochs\n",
      "  |-- [Party 12] Average Train Loss: 0.4545 ... 20 local epochs\n",
      "  |-- [Party 13] Average Train Loss: 0.4314 ... 20 local epochs\n",
      "  |-- [Party 14] Average Train Loss: 0.4273 ... 20 local epochs\n",
      "  |-- [Party 15] Average Train Loss: 0.4794 ... 20 local epochs\n",
      "  |-- [Party 16] Average Train Loss: 0.4387 ... 20 local epochs\n",
      "    |---- Test Accuracy: 15.4800%\n",
      "    |---- Test Loss: 5.1829\n",
      "    |---- Elapsed time: 0:10:50.968016\n",
      "\n",
      " | Global Training Round : 2 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.2348 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.2429 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.2445 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.2289 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.2455 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.2407 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.2373 ... 20 local epochs\n",
      "  |-- [Party  8] Average Train Loss: 0.2386 ... 20 local epochs\n",
      "  |-- [Party  9] Average Train Loss: 0.2321 ... 20 local epochs\n",
      "  |-- [Party 10] Average Train Loss: 0.2330 ... 20 local epochs\n",
      "  |-- [Party 11] Average Train Loss: 0.2330 ... 20 local epochs\n",
      "  |-- [Party 12] Average Train Loss: 0.2364 ... 20 local epochs\n",
      "  |-- [Party 13] Average Train Loss: 0.2366 ... 20 local epochs\n",
      "  |-- [Party 14] Average Train Loss: 0.2388 ... 20 local epochs\n",
      "  |-- [Party 15] Average Train Loss: 0.2541 ... 20 local epochs\n",
      "  |-- [Party 16] Average Train Loss: 0.2357 ... 20 local epochs\n",
      "    |---- Test Accuracy: 18.8700%\n",
      "    |---- Test Loss: 5.0321\n",
      "    |---- Elapsed time: 0:21:35.393371\n",
      "\n",
      " | Global Training Round : 3 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.1322 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.1414 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.1506 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.1372 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.1443 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.1377 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.1420 ... 20 local epochs\n",
      "  |-- [Party  8] Average Train Loss: 0.1348 ... 20 local epochs\n",
      "  |-- [Party  9] Average Train Loss: 0.1308 ... 20 local epochs\n",
      "  |-- [Party 10] Average Train Loss: 0.1404 ... 20 local epochs\n",
      "  |-- [Party 11] Average Train Loss: 0.1408 ... 20 local epochs\n",
      "  |-- [Party 12] Average Train Loss: 0.1395 ... 20 local epochs\n",
      "  |-- [Party 13] Average Train Loss: 0.1392 ... 20 local epochs\n",
      "  |-- [Party 14] Average Train Loss: 0.1429 ... 20 local epochs\n",
      "  |-- [Party 15] Average Train Loss: 0.1378 ... 20 local epochs\n",
      "  |-- [Party 16] Average Train Loss: 0.1377 ... 20 local epochs\n",
      "    |---- Test Accuracy: 53.4000%\n",
      "    |---- Test Loss: 1.9685\n",
      "    |---- Elapsed time: 0:32:21.536757\n",
      "\n",
      " | Global Training Round : 4 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.0919 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.0991 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.0953 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.0971 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.0962 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.0929 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.0931 ... 20 local epochs\n",
      "  |-- [Party  8] Average Train Loss: 0.0967 ... 20 local epochs\n",
      "  |-- [Party  9] Average Train Loss: 0.0823 ... 20 local epochs\n",
      "  |-- [Party 10] Average Train Loss: 0.0955 ... 20 local epochs\n",
      "  |-- [Party 11] Average Train Loss: 0.0934 ... 20 local epochs\n",
      "  |-- [Party 12] Average Train Loss: 0.0965 ... 20 local epochs\n",
      "  |-- [Party 13] Average Train Loss: 0.0954 ... 20 local epochs\n",
      "  |-- [Party 14] Average Train Loss: 0.0958 ... 20 local epochs\n",
      "  |-- [Party 15] Average Train Loss: 0.1010 ... 20 local epochs\n",
      "  |-- [Party 16] Average Train Loss: 0.0956 ... 20 local epochs\n",
      "    |---- Test Accuracy: 66.6000%\n",
      "    |---- Test Loss: 1.3983\n",
      "    |---- Elapsed time: 0:43:09.258803\n",
      "\n",
      " | Global Training Round : 5 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.0713 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.0759 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.0760 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.0723 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.0716 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.0753 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.0738 ... 20 local epochs\n",
      "  |-- [Party  8] Average Train Loss: 0.0711 ... 20 local epochs\n",
      "  |-- [Party  9] Average Train Loss: 0.0693 ... 20 local epochs\n",
      "  |-- [Party 10] Average Train Loss: 0.0747 ... 20 local epochs\n",
      "  |-- [Party 11] Average Train Loss: 0.0753 ... 20 local epochs\n",
      "  |-- [Party 12] Average Train Loss: 0.0723 ... 20 local epochs\n",
      "  |-- [Party 13] Average Train Loss: 0.0772 ... 20 local epochs\n",
      "  |-- [Party 14] Average Train Loss: 0.0714 ... 20 local epochs\n",
      "  |-- [Party 15] Average Train Loss: 0.0709 ... 20 local epochs\n",
      "  |-- [Party 16] Average Train Loss: 0.0747 ... 20 local epochs\n",
      "    |---- Test Accuracy: 70.6200%\n",
      "    |---- Test Loss: 1.2025\n",
      "    |---- Elapsed time: 0:53:56.661227\n",
      "\n",
      " | Global Training Round : 6 / 50 |\n",
      "  |-- [Party  1] Average Train Loss: 0.0584 ... 20 local epochs\n",
      "  |-- [Party  2] Average Train Loss: 0.0580 ... 20 local epochs\n",
      "  |-- [Party  3] Average Train Loss: 0.0594 ... 20 local epochs\n",
      "  |-- [Party  4] Average Train Loss: 0.0588 ... 20 local epochs\n",
      "  |-- [Party  5] Average Train Loss: 0.0595 ... 20 local epochs\n",
      "  |-- [Party  6] Average Train Loss: 0.0587 ... 20 local epochs\n",
      "  |-- [Party  7] Average Train Loss: 0.0625 ... 20 local epochs\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_accs, test_losses = [], [], []\n",
    "\n",
    "st = time.time()\n",
    "for r in range(ROUNDS):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {r + 1} / {ROUNDS} |')\n",
    "    \n",
    "    fed_model.train()\n",
    "    \n",
    "    for i, p in enumerate(parties):\n",
    "        w, ls = train(copy.deepcopy(fed_model), trainloaders[i], EPOCHS, LR, MOMENTUM, WEIGHT_DECAY, criterion, device)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        print('  |-- [Party {:>2}] Average Train Loss: {:.4f} ... {} local epochs'.format(i + 1, sum(ls) / len(ls), EPOCHS))\n",
    "        \n",
    "    fed_weights = average_weights(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    \n",
    "    test_acc, test_ls = test(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    print('    |---- Test Accuracy: {:.4f}%'.format(100 * test_acc))\n",
    "    print('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    print('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DSET, '{}_tr_ls.npy'.format(FNAME)), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(SAVE_DSET, '{}_te_ls.npy'.format(FNAME)), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(SAVE_DSET, '{}_te_acc.npy'.format(FNAME)), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 9))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='purple')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_losses.mean(axis=1), c='red')\n",
    "axs[2].set_title('Train Average Losses')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Loss')\n",
    "axs[3].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='blue')\n",
    "axs[3].set_title('Train Average Losses')\n",
    "axs[3].set_xlabel('Rounds')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
