{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "torch.manual_seed = seed\n",
    "print('Random seed: ', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = os.path.dirname(os.getcwd())\n",
    "PATH_DATA = os.path.join(PATH_ROOT, 'data')\n",
    "\n",
    "print('Project root: ', PATH_ROOT)\n",
    "print('Project data: ', PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.cifar10 = dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.cifar10[idx]\n",
    "        return data, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = 0.5, 0.5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean, mean, mean), (std, std, std))\n",
    "])\n",
    "\n",
    "cifar10_dataset = datasets.CIFAR10(PATH_DATA, train=True, transform=transform, download=True)\n",
    "custom_dataset = CustomDataset(cifar10_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 ** 6\n",
    "NUM_WORKERS = 0\n",
    "print('Batch size: ', BATCH_SIZE)\n",
    "print('Num workers: ', NUM_WORKERS)\n",
    "    \n",
    "trainloader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_planes != self.expansion * planes):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_planes != self.expansion * planes):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3, 64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Resume from Here ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'indices': np.arange(len(custom_dataset)),\n",
    "     'targets': cifar10_dataset.targets}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df['predictions'] = 0\n",
    "df['learn'] = 0\n",
    "df['forget'] = 0\n",
    "df['forgettable'] = 0\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "df_fed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "NESTEROV = True\n",
    "# MILESTONES = [60, 120, 160]\n",
    "# GAMMA = 0.2\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=NESTEROV)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, images, labels):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(df, indices, preds, learn):\n",
    "    df.loc[indices, 'predictions'] = preds.detach().to('cpu').numpy()\n",
    "    if learn:\n",
    "        df.loc[(df.index.isin(indices)) & (df['learn'] == 0) & (df['targets'] == df['predictions']), 'learn'] += 1\n",
    "    else:\n",
    "        df.loc[(df.index.isin(indices)) & (df['learn'] > 0) & (df['targets'] != df['predictions']), 'forget'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, images, labels, indices, df, learn):\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    preds = preds.view(-1)\n",
    "    \n",
    "    record(df, indices, preds, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "st = time.time()\n",
    "for ep in range(EPOCHS):\n",
    "    print(f'[Epoch {ep + 1} / {EPOCHS}]')\n",
    "    \n",
    "    curr_images = []\n",
    "    curr_labels = []\n",
    "    curr_indices = []\n",
    "    \n",
    "    for batch_idx, (images, labels, indices) in enumerate(trainloader):\n",
    "        curr_images.append(images)\n",
    "        curr_labels.append(labels)\n",
    "        curr_indices.append(indices)\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model = train(model, optimizer, criterion, images, labels)\n",
    "        test(model, images, labels, indices, df, learn=True)\n",
    "        if ep > 0:\n",
    "            test(model, prev_images[batch_idx].to(device), prev_labels[batch_idx].to(device), prev_indices[batch_idx], df, learn=False)\n",
    "            \n",
    "    prev_images = curr_images.copy()\n",
    "    prev_labels = curr_labels.copy()\n",
    "    prev_indices = curr_indices.copy()\n",
    "    \n",
    "    print(f'| Learning Events: {df[\"learn\"].sum()} | Forgetting Events: {df[\"forget\"].sum()}')\n",
    "    print(f'|-- Elapsed time: {timedelta(seconds=time.time()-st)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['forget'] > 0), 'forgettable'] = 1\n",
    "df.loc[(df['learn'] == 0) & (df['forget'] == 0), 'forgettable'] = 1\n",
    "\n",
    "print('Number of Forgettable Samples')\n",
    "df['forgettable'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTIES = 10\n",
    "\n",
    "indices = list(range(len(custom_dataset)))\n",
    "random.shuffle(indices)\n",
    "parties = list(chunks(indices, int(len(custom_dataset) / NUM_PARTIES)))\n",
    "\n",
    "trainloaders = []\n",
    "for p in parties:\n",
    "    train_subset = Subset(custom_dataset, p)\n",
    "    trainloaders.append(\n",
    "        DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUNDS = EPOCHS\n",
    "\n",
    "fed_model = ResNet18().to(device)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "curr_images, curr_labels, curr_indices = {}, {}, {}\n",
    "prev_images, prev_labels, prev_indices = {}, {}, {}\n",
    "\n",
    "st = time.time()\n",
    "for r in range(ROUNDS):\n",
    "    print(f'[Round {r + 1} / {ROUNDS}]')\n",
    "    local_weights = []\n",
    "    \n",
    "    for i in range(NUM_PARTIES):\n",
    "        curr_images[i] = []\n",
    "        curr_labels[i] = []\n",
    "        curr_indices[i] = []\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        optimizer = optim.SGD(local_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=NESTEROV)\n",
    "\n",
    "        for batch_idx, (images, labels, indices) in enumerate(trainloaders[i]):\n",
    "            curr_images[i].append(images)\n",
    "            curr_labels[i].append(labels)\n",
    "            curr_indices[i].append(indices)\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            local_model = train(local_model, optimizer, criterion, images, labels)\n",
    "            test(local_model, images, labels, indices, df_fed, learn=True)\n",
    "            if r > 0:\n",
    "                test(local_model, prev_images[i][batch_idx].to(device), prev_labels[i][batch_idx].to(device), prev_indices[i][batch_idx], df_fed, learn=False)\n",
    "\n",
    "        prev_images[i] = curr_images[i].copy()\n",
    "        prev_labels[i] = curr_labels[i].copy()\n",
    "        prev_indices[i] = curr_indices[i].copy()\n",
    "        \n",
    "        local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "        print('|---- [Party {:>2}] Complete'.format(i + 1))\n",
    "        \n",
    "    fed_weights = average_weights(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "\n",
    "    print(f'| Learning Events: {df_fed[\"learn\"].sum()} | Forgetting Events: {df_fed[\"forget\"].sum()}')\n",
    "    print(f'|-- Elapsed time: {timedelta(seconds=time.time()-st)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fed.loc[(df_fed['forget'] > 0), 'forgettable'] = 1\n",
    "df_fed.loc[(df_fed['learn'] == 0) & (df_fed['forget'] == 0), 'forgettable'] = 1\n",
    "\n",
    "print('Number of Forgettable Samples')\n",
    "df_fed['forgettable'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['forgettable'].value_counts())\n",
    "print(df_fed['forgettable'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
