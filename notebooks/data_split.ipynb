{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "PSET = [12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56]\n",
    "NUM_PARTIES = PSET[11]    # 0(12) ~ 11(56)\n",
    "\n",
    "# DATASETS = ['cifar10', 'mnist', 'permuted_mnist', 'fmnist']\n",
    "DATASETS = ['cifar10', 'mnist', 'fmnist']\n",
    "\n",
    "IFD, STRATIFY = True, True\n",
    "# IFD, STRATIFY = True, False\n",
    "# IFD, STRATIFY = False, False\n",
    "# IFD, STRATIFY = False, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= #\n",
    "# SEED = 42 #\n",
    "# ========= #\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed = SEED\n",
    "np.random.seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = os.path.dirname(os.getcwd())\n",
    "PATH_DATA = os.path.join(PATH_ROOT, 'data')\n",
    "\n",
    "print('Root directory: ', PATH_ROOT)\n",
    "print('Data directory: ', PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================== #\n",
    "# DATASETS = ['cifar10', 'mnist', 'permuted_mnist', 'fmnist'] #\n",
    "# =========================================================== #\n",
    "\n",
    "dfdict = {}\n",
    "\n",
    "for ds in tqdm(DATASETS):\n",
    "    print('| {} |'.format(ds.upper()))\n",
    "    if ds == 'cifar10':\n",
    "        train_dataset = datasets.CIFAR10(PATH_DATA, train=True, download=True)\n",
    "    elif ds == 'mnist':\n",
    "        train_dataset = datasets.MNIST(PATH_DATA, train=True, download=True)\n",
    "    elif ds == 'permuted_mnist':\n",
    "        continue\n",
    "    elif ds == 'fmnist':\n",
    "        train_dataset = datasets.FashionMNIST(PATH_DATA, train=True, download=True)\n",
    "\n",
    "    with open(os.path.join(PATH_DATA, '{}_sorted.pkl'.format(ds)), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print('|-- Keys: ', data.keys())\n",
    "    print('|-- Size: ', len(data['indices']))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.columns = ['indices', 'forgetting counts']\n",
    "    df = df.sort_values('indices').reset_index(drop=True)\n",
    "    df['forgettable'] = df['forgetting counts'] > 0\n",
    "    df['forgettable'] = df['forgettable'].astype(np.int)\n",
    "    df['targets'] = train_dataset.targets\n",
    "\n",
    "    print('|-- Forgettables: ')\n",
    "    print(df['forgettable'].value_counts())\n",
    "    display(df.head())\n",
    "    \n",
    "    df.to_csv(os.path.join(PATH_DATA, '{}_flagged.csv'.format(ds)), index=False, encoding='utf-8')\n",
    "    \n",
    "    dfdict[ds] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, df in tqdm(dfdict.items()):\n",
    "    forget_df = df.loc[df['forgettable'] == 1]\n",
    "    unforget_df = df.loc[df['forgettable'] == 0]\n",
    "\n",
    "    print('| {} |'.format(ds.upper()))\n",
    "    print('|-- Forgettable samples: ', len(forget_df.index))\n",
    "    print('|-- Unforgettable samples: ', len(unforget_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== #\n",
    "# NUM_PARTIES = 8 #\n",
    "# IFD = True      #\n",
    "# STRATIFY = True #\n",
    "# =============== #\n",
    "\n",
    "dirdict = {}\n",
    "\n",
    "for ds, df in tqdm(dfdict.items()):\n",
    "    print('| {} |'.format(ds.upper()))\n",
    "    PATH_DSET = os.path.join(PATH_DATA, '{}'.format(ds))\n",
    "    if not os.path.exists(PATH_DSET):\n",
    "        os.mkdir(PATH_DSET)\n",
    "    print('|-- Directory: '.format(ds.upper()), PATH_DSET)\n",
    "    \n",
    "    PATH_PART = os.path.join(PATH_DSET, '{}parties'.format(NUM_PARTIES))\n",
    "    if not os.path.exists(PATH_PART):\n",
    "        os.mkdir(PATH_PART)\n",
    "    print('|-- {}-party directory: '.format(NUM_PARTIES), PATH_PART)\n",
    "    \n",
    "    if IFD:\n",
    "        PATH_FGD = os.path.join(PATH_PART, 'ifd')\n",
    "        print('|-- Identical forgettable distribution directory: ', PATH_FGD)\n",
    "    else:\n",
    "        PATH_FGD = os.path.join(PATH_PART, 'non_ifd')\n",
    "        print('|-- Non-identical forgettable distribution directory: ', PATH_FGD)\n",
    "    if not os.path.exists(PATH_FGD):\n",
    "        os.mkdir(PATH_FGD)\n",
    "    \n",
    "    if STRATIFY:\n",
    "        PATH_TGD = os.path.join(PATH_FGD, 'stratified')\n",
    "        print('|-- Stratified label distribution directory: ', PATH_TGD)\n",
    "    else:\n",
    "        PATH_TGD = os.path.join(PATH_FGD, 'random')\n",
    "        print('|-- Random label distribution directory: ', PATH_TGD)\n",
    "    if not os.path.exists(PATH_TGD):\n",
    "        os.mkdir(PATH_TGD)\n",
    "        \n",
    "    dirdict[ds] = PATH_TGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== #\n",
    "# SEED = 42       #\n",
    "# NUM_PARTIES = 8 #\n",
    "# IFD = True      #\n",
    "# STRATIFY = True #\n",
    "# =============== #\n",
    "\n",
    "for ds, df in tqdm(dfdict.items()):\n",
    "    savedir = dirdict[ds]\n",
    "    print('| {} |'.format(savedir.upper()))\n",
    "    \n",
    "    parties = []\n",
    "\n",
    "    if IFD:\n",
    "        forget_df = df.loc[df['forgettable'] == 1]\n",
    "        unforget_df = df.loc[df['forgettable'] == 0]\n",
    "\n",
    "        forget_items = int(len(forget_df.index) / NUM_PARTIES)\n",
    "        unforget_items = int(len(unforget_df.index) / NUM_PARTIES)\n",
    "\n",
    "        tmp_df1 = forget_df.copy()\n",
    "        tmp_df2 = unforget_df.copy()\n",
    "\n",
    "        for i in range(NUM_PARTIES - 1):\n",
    "            if STRATIFY:\n",
    "                f, _ = train_test_split(tmp_df1['indices'], train_size=forget_items, random_state=SEED, shuffle=True, stratify=tmp_df1['targets'])\n",
    "                u, _ = train_test_split(tmp_df2['indices'], train_size=unforget_items, random_state=SEED, shuffle=True, stratify=tmp_df2['targets'])\n",
    "            else:\n",
    "                f, _ = train_test_split(tmp_df1['indices'], train_size=forget_items, random_state=SEED, shuffle=True)\n",
    "                u, _ = train_test_split(tmp_df2['indices'], train_size=unforget_items, random_state=SEED, shuffle=True)\n",
    "            tmp_df1 = tmp_df1.drop(f)\n",
    "            tmp_df2 = tmp_df2.drop(u)\n",
    "            p = pd.concat([f, u])\n",
    "            p = p.to_numpy()\n",
    "            f = f.to_numpy()\n",
    "            u = u.to_numpy()\n",
    "            d = {'all': p, 'forget': f, 'unforget': u}\n",
    "            parties.append(d)\n",
    "\n",
    "        f = tmp_df1['indices']\n",
    "        u = tmp_df2['indices']\n",
    "        p = pd.concat([f, u])\n",
    "        p = p.to_numpy()\n",
    "        f = f.to_numpy()\n",
    "        u = u.to_numpy()\n",
    "        d = {'all': p, 'forget': f, 'unforget': u}\n",
    "        parties.append(d)\n",
    "\n",
    "        # In case not equally divided\n",
    "        # parties[-1]['forget'] = np.append(parties[-1]['forget'], parties[-2]['forget'][-1])\n",
    "        # parties[-2]['forget'] = parties[-2]['forget'][:-1]\n",
    "        # parties[-2]['unforget'] = np.append(parties[-2]['unforget'], parties[-1]['unforget'][-1])\n",
    "        # parties[-1]['unforget'] = parties[-1]['unforget'][:-1]\n",
    "        # parties[-1]['all'] = np.concatenate([parties[-1]['forget'], parties[-1]['unforget']])\n",
    "        # parties[-2]['all'] = np.concatenate([parties[-2]['forget'], parties[-2]['unforget']])\n",
    "    else:\n",
    "        tmp_df = df.copy()\n",
    "        num_items = int(len(tmp_df.index) / NUM_PARTIES)\n",
    "        \n",
    "        for i in range(NUM_PARTIES - 1):\n",
    "            if STRATIFY:\n",
    "                p, _ = train_test_split(tmp_df['indices'], train_size=num_items, random_state=SEED, shuffle=True, stratify=tmp_df['targets'])\n",
    "            else:\n",
    "                p, _ = train_test_split(tmp_df['indices'], train_size=num_items, random_state=SEED, shuffle=True)\n",
    "            tmp_df = tmp_df.drop(p)\n",
    "            p = p.to_numpy()\n",
    "            f = df.iloc[p].loc[df['forgettable'] == 1, 'indices'].to_numpy()\n",
    "            u = df.iloc[p].loc[df['forgettable'] == 0, 'indices'].to_numpy()\n",
    "            d = {'all': p, 'forget': f, 'unforget': u}\n",
    "            parties.append(d)\n",
    "            \n",
    "        p = tmp_df['indices']\n",
    "        p = p.to_numpy()\n",
    "        f = df.iloc[p].loc[df['forgettable'] == 1, 'indices'].to_numpy()\n",
    "        u = df.iloc[p].loc[df['forgettable'] == 0, 'indices'].to_numpy()\n",
    "        d = {'all': p, 'forget': f, 'unforget': u}\n",
    "        parties.append(d)\n",
    "\n",
    "    for i, p in enumerate(parties):\n",
    "        print('|-- [{:>2}]'.format(i + 1), p['all'].shape, p['forget'].shape, p['unforget'].shape)\n",
    "        tmp_df = df.iloc[p['all']]\n",
    "        tmp_df.to_csv(os.path.join(savedir, '{}_p{}.csv'.format(ds, i + 1)), index=False, encoding='utf-8')\n",
    "        print('|---- Saved as: ', '{}_p{}.csv'.format(ds, i + 1))\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
